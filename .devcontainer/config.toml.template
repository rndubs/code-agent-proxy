# OpenAI Codex Configuration Template
# This file is auto-generated by .devcontainer/setup-codex.sh
# DO NOT EDIT MANUALLY - Edit .env instead and rebuild the devcontainer

# Default model to use
model = "gpt-4"

# Provider configuration - points to LiteLLM proxy
model_provider = "openai"

[model_providers.openai]
name = "LiteLLM Proxy"
# This points to the local LiteLLM proxy, which routes to third-party APIs
base_url = "http://litellm:4000/v1"
# This uses the LITELLM_MASTER_KEY from environment
env_key = "OPENAI_API_KEY"
# Use chat completions API
wire_api = "chat"

# Optional: Add other model providers here
# [model_providers.anthropic]
# name = "Anthropic via LiteLLM"
# base_url = "http://litellm:4000/v1"
# env_key = "OPENAI_API_KEY"
# wire_api = "chat"
