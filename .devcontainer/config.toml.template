# OpenAI Codex Configuration Template
# This file is auto-generated by .devcontainer/setup-codex.sh
# DO NOT EDIT MANUALLY - Edit .env instead and rebuild the devcontainer

# ============================================================================
# PRIVACY & TELEMETRY SETTINGS
# ============================================================================
# All telemetry is DISABLED by default for privacy

[otel]
# Disable all telemetry/analytics collection
# Options: "none" (disabled) | "otlp-http" | "otlp-grpc"
exporter = "none"

# Never log user prompts to telemetry (even if telemetry were enabled)
log_user_prompt = false

# Environment label for telemetry (not sent when exporter = "none")
environment = "dev"

# ============================================================================
# MODEL CONFIGURATION
# ============================================================================

# Default model to use
model = "gpt-4"

# Provider configuration - points to LiteLLM proxy
model_provider = "openai"

[model_providers.openai]
name = "LiteLLM Proxy"
# This points to the local LiteLLM proxy, which routes to third-party APIs
base_url = "http://litellm:4000/v1"
# This uses the LITELLM_MASTER_KEY from environment
env_key = "OPENAI_API_KEY"
# Use chat completions API
wire_api = "chat"

# ============================================================================
# OPTIONAL: ADDITIONAL MODEL PROVIDERS
# ============================================================================
# You can add more providers that also route through LiteLLM

# [model_providers.anthropic]
# name = "Anthropic via LiteLLM"
# base_url = "http://litellm:4000/v1"
# env_key = "OPENAI_API_KEY"
# wire_api = "chat"

